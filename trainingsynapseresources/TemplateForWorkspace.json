{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "trainingsynapseresources"
		},
		"CosmosDb1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'CosmosDb1'"
		},
		"trainingsynapseresources-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'trainingsynapseresources-WorkspaceDefaultSqlServer'"
		},
		"AzureMLService1_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "924cf5c7-5f72-42ba-98ee-7ea905016982"
		},
		"AzureMLService1_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "trainingresourcegroupv10"
		},
		"trainingstoragev101_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://trainingstoragev101.dfs.core.windows.net/"
		},
		"trainingsynapseresources-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://synapsedlsv10.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/AzureMLService1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureMLService",
				"typeProperties": {
					"subscriptionId": "[parameters('AzureMLService1_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('AzureMLService1_properties_typeProperties_resourceGroupName')]",
					"mlWorkspaceName": "trainingmlworkspacev10",
					"authentication": "MSI"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CosmosDb1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDb",
				"typeProperties": {
					"connectionString": "[parameters('CosmosDb1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/trainingstoragev101')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('trainingstoragev101_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/trainingsynapseresources-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('trainingsynapseresources-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/trainingsynapseresources-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('trainingsynapseresources-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TrainingDataFlowLibrary')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "UDFLibrary",
				"typeProperties": {
					"sources": [],
					"sinks": [],
					"transformations": [],
					"scriptLines": [
						"ExConcat(string, string) as string = concat(i1,i2)"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Databricks - Query Performance with Parquet and Delta')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool001v1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0f07fe79-db9b-4be5-a9fa-2651c3f9b53d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/924cf5c7-5f72-42ba-98ee-7ea905016982/resourceGroups/trainingresourcegroupv10/providers/Microsoft.Synapse/workspaces/trainingsynapseresources/bigDataPools/SparkPool001v1",
						"name": "SparkPool001v1",
						"type": "Spark",
						"endpoint": "https://trainingsynapseresources.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool001v1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"mssparkutils.fs.mount(\r\n",
							"    \"abfss://data@trainingstoragev101.dfs.core.windows.net/\",\r\n",
							"    \"/test\",\r\n",
							"    {\r\n",
							"        \"linkedService\": \"trainingstoragev101\"\r\n",
							"    }\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jobId = mssparkutils.env.getJobId()\r\n",
							"path = \"synfs:/\" + jobId + \"/test/data.csv\"\r\n",
							"\r\n",
							"print(path)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"flights = spark \\\r\n",
							"    .read \\\r\n",
							"    .format(\"csv\") \\\r\n",
							"    .option(\"header\", \"true\") \\\r\n",
							"    .option(\"inferSchema\", \"true\") \\\r\n",
							"    .load(path)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"flights.printSchema()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"flights \\\r\n",
							"    .write \\\r\n",
							"    .format(\"parquet\") \\\r\n",
							"    .mode(\"overwrite\") \\\r\n",
							"    .partitionBy(\"Origin\") \\\r\n",
							"    .save(\"/tmp/flights_parquet\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import count\r\n",
							"\r\n",
							"flights_parquet = \\\r\n",
							"    spark \\\r\n",
							"        .read \\\r\n",
							"        .format(\"parquet\") \\\r\n",
							"        .load(\"/tmp/flights_parquet\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(\r\n",
							"    flights_parquet\r\n",
							"        .filter(\"DayOfWeek = 1\")\r\n",
							"        .groupBy(\"Month\", \"Origin\")\r\n",
							"        .agg(count(\"*\").alias(\"TotalFlights\"))\r\n",
							"        .orderBy(\"TotalFlights\", ascending = False)\r\n",
							"        .limit(20)\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"flights \\\r\n",
							"    .write \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .mode(\"overwrite\") \\\r\n",
							"    .partitionBy(\"Origin\") \\\r\n",
							"    .save(\"/tmp/flights_delta\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"DROP TABLE IF EXISTS flights;"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"CREATE TABLE flights\r\n",
							"USING DELTA\r\n",
							"LOCATION '/tmp/flights_delta';"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"OPTIMIZE flights ZORDER BY (DayOfWeek);"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"flights_deltav2 = \\\r\n",
							"    spark \\\r\n",
							"        .read \\\r\n",
							"        .format(\"delta\") \\\r\n",
							"        .load('/tmp/flights_delta')"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(\r\n",
							"    flights_deltav2\r\n",
							"        .filter(\"DayOfWeek = 1\")\r\n",
							"        .groupBy(\"Month\", \"Origin\")\r\n",
							"        .agg(count(\"*\").alias(\"TotalFlights\"))\r\n",
							"        .orderBy(\"TotalFlights\", ascending = False)\r\n",
							"        .limit(20)\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta Lake Exploration')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool001v1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0c64b8f1-b44c-4bcb-8198-daa35f44e04b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/924cf5c7-5f72-42ba-98ee-7ea905016982/resourceGroups/trainingresourcegroupv10/providers/Microsoft.Synapse/workspaces/trainingsynapseresources/bigDataPools/SparkPool001v1",
						"name": "SparkPool001v1",
						"type": "Spark",
						"endpoint": "https://trainingsynapseresources.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool001v1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import random\r\n",
							"\r\n",
							"session_id = random.randint(1,1000000)\r\n",
							"delta_table_path = \"/delta-training/delta-table-{0}\".format(session_id)\r\n",
							"\r\n",
							"delta_table_path"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data = spark.range(0,10)\r\n",
							"data.show()\r\n",
							"\r\n",
							"data \\\r\n",
							"    .write \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark \\\r\n",
							"    .read \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .load(delta_table_path)\r\n",
							"\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data = spark.range(10, 20)\r\n",
							"data \\\r\n",
							"    .write \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .mode(\"overwrite\") \\\r\n",
							"    .save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data \\\r\n",
							"    .write \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .saveAsTable(\"ManagedDeltaTablev2\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"CREATE TABLE ExternalDeltaTablev2\r\n",
							"USING DELTA\r\n",
							"LOCATION '/delta-training/delta-table-645199'"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SHOW TABLES"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"DESCRIBE EXTENDED manageddeltatablev2"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"delta_table.update(\r\n",
							"    condition = expr (\"id % 2 == 1\"),\r\n",
							"    set = { \"id\": expr(\"id + 100\")}\r\n",
							")\r\n",
							"\r\n",
							"delta_table.delete(\"id >= 9\")\r\n",
							"\r\n",
							"delta_table.toDF().show()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data = spark.range(10, 20)\r\n",
							"data \\\r\n",
							"    .write \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .mode(\"overwrite\") \\\r\n",
							"    .save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"new_data = spark.range(15, 25)\r\n",
							"\r\n",
							"delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"\r\n",
							"delta_table \\\r\n",
							"    .alias(\"oldData\") \\\r\n",
							"    .merge(new_data.alias(\"newData\"), \"oldData.id = newData.id\") \\\r\n",
							"    .whenMatchedUpdate(set = {'id': lit(-1)}) \\\r\n",
							"    .whenNotMatchedInsert(values = { 'id': col(\"newData.id\") }) \\\r\n",
							"    .execute()\r\n",
							"\r\n",
							"delta_table.toDF().show()"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"delta_table \\\r\n",
							"    .history() \\\r\n",
							"    .show()"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark \\\r\n",
							"    .read \\\r\n",
							"    .format (\"delta\") \\\r\n",
							"    .option(\"versionAsOf\", 0) \\\r\n",
							"    .load(delta_table_path)\r\n",
							"\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"streaming_df = \\\r\n",
							"    spark \\\r\n",
							"        .readStream \\\r\n",
							"        .format(\"rate\") \\\r\n",
							"        .load()\r\n",
							"    \r\n",
							"stream = streaming_df \\\r\n",
							"    .selectExpr(\"value as id\") \\\r\n",
							"    .writeStream \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .option(\"checkpointLocation\", \"/tmp/checkpoint-{0}\".format(session_id)) \\\r\n",
							"    .start(delta_table_path)\r\n",
							"\r\n",
							"stream.status"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"delta_table \\\r\n",
							"    .toDF() \\\r\n",
							"    .sort(col(\"id\").desc()) \\\r\n",
							"    .show(100)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"delta_table \\\r\n",
							"    .history() \\\r\n",
							"    .show()"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stream.stop()"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"partition_count = 2\r\n",
							"\r\n",
							"spark \\\r\n",
							"    .read \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .load(delta_table_path) \\\r\n",
							"    .repartition(partition_count) \\\r\n",
							"    .write \\\r\n",
							"    .option(\"dataChange\", \"false\") \\\r\n",
							"    .mode(\"overwrite\") \\\r\n",
							"    .save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"DESCRIBE HISTORY ManagedDeltaTablev2\").show()"
						],
						"outputs": [],
						"execution_count": 26
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta Other Optimization Techniques')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool001v1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1704984f-d411-45e8-a764-b83ff8905fe1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/924cf5c7-5f72-42ba-98ee-7ea905016982/resourceGroups/trainingresourcegroupv10/providers/Microsoft.Synapse/workspaces/trainingsynapseresources/bigDataPools/SparkPool001v1",
						"name": "SparkPool001v1",
						"type": "Spark",
						"endpoint": "https://trainingsynapseresources.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool001v1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"mssparkutils.fs.mount(\r\n",
							"    \"abfss://data@trainingstoragev101.dfs.core.windows.net/\",\r\n",
							"    \"/test\",\r\n",
							"    {\r\n",
							"        \"linkedService\": \"trainingstoragev101\"\r\n",
							"    }\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jobId = mssparkutils.env.getJobId()\r\n",
							"path = \"synfs:/\" + jobId + \"/test/Common/Orders/parquetFiles\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.ls(path)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import count, year\r\n",
							"\r\n",
							"ordersDF = spark \\\r\n",
							"    .read \\\r\n",
							"    .format(\"parquet\") \\\r\n",
							"    .load(path) \\\r\n",
							"    .withColumn(\"OrderYear\", year(\"O_ORDERDATE\"))\r\n",
							"\r\n",
							"display(ordersDF.limit(5))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(\r\n",
							"    ordersDF\r\n",
							"        .filter(\"O_ORDERSTATUS = 'O'\")\r\n",
							"        .groupBy(\"OrderYear\", \"O_ORDERPRIORITY\")\r\n",
							"        .agg(count(\"*\").alias(\"TotalOrders\"))\r\n",
							"        .orderBy(\"TotalOrders\", ascending = False)\r\n",
							"        .limit(20)\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ordersDF \\\r\n",
							"    .write \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .mode(\"overwrite\") \\\r\n",
							"    .partitionBy(\"OrderYear\") \\\r\n",
							"    .save(\"/tmp/orders-delta\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"CREATE TABLE Orders\r\n",
							"USING DELTA\r\n",
							"LOCATION \"/tmp/orders-delta\""
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"OPTIMIZE Orders ZORDER BY (O_ORDERPRIORITY)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT COUNT(*) AS TotalOrders FROM Orders\r\n",
							"WHERE O_ORDERSTATUS = 'O'\r\n",
							"GROUP BY OrderYear, O_ORDERPRIORITY\r\n",
							"ORDER BY TotalOrders DESC\r\n",
							"LIMIT 20"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"VACUUM Orders "
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"DESCRIBE HISTORY Orders"
						],
						"outputs": [],
						"execution_count": 17
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Hyperspace Integration with Spark Pool')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool001v1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3a796592-7515-4c03-8bed-2589e99e2428"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/924cf5c7-5f72-42ba-98ee-7ea905016982/resourceGroups/trainingresourcegroupv10/providers/Microsoft.Synapse/workspaces/trainingsynapseresources/bigDataPools/SparkPool001v1",
						"name": "SparkPool001v1",
						"type": "Spark",
						"endpoint": "https://trainingsynapseresources.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool001v1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import random\r\n",
							"session_id = random.randint(1,1000000)\r\n",
							"data_path = \"/hyperspace/data-{0}\".format(session_id)\r\n",
							"index_location = \"/hyperspace/indexes-{0}\".format(session_id)\r\n",
							"\r\n",
							"spark.conf.set(\"spark.hyperspace.system.path\", index_location)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\r\n",
							"spark.conf.set(\"spark.hyperspace.explain.displayMode\", \"html\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.types import *\r\n",
							"\r\n",
							"departments = [(10, \"Accounting\", \"Bangalore\"), (11, \"Marketing\", \"Singapore\"), (12, \"IT\", \"New York\"), (13, \"HR\", \"Bangalore\"),\r\n",
							"    (14, \"Personnel\", \"Chennai\")]\r\n",
							"\r\n",
							"employees = [(1, \"Ramkumar\", 10), \r\n",
							"    (2, \"Rajkumar\", 10),\r\n",
							"    (3, \"Rajesh\", 11),\r\n",
							"    (4, \"Mahesh\", 10),\r\n",
							"    (5, \"Mukhesh\", 10),\r\n",
							"    (6, \"Vikram\", 12),\r\n",
							"    (7, \"Lilly\", 12),\r\n",
							"    (8, \"Sophana\", 13),\r\n",
							"    (9, \"Brown\", 10),\r\n",
							"    (10, \"Mathais\", 14)]\r\n",
							"\r\n",
							"depart_schema = StructType([\r\n",
							"    StructField(\"deptId\", IntegerType(), True),\r\n",
							"    StructField(\"deptName\", StringType(), True),\r\n",
							"    StructField(\"location\", StringType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"emp_schema = StructType([\r\n",
							"    StructField(\"empId\", IntegerType(), True),\r\n",
							"    StructField(\"empName\", StringType(), True),\r\n",
							"    StructField(\"deptId\", IntegerType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"departments_df = spark.createDataFrame(departments, depart_schema)\r\n",
							"employees_df = spark.createDataFrame(employees, emp_schema)\r\n",
							"\r\n",
							"dept_location = data_path + \"/departments.parquet\"\r\n",
							"emp_location = data_path + \"/employees.parquet\"\r\n",
							"\r\n",
							"departments_df.write.mode(\"overwrite\").parquet(dept_location)\r\n",
							"employees_df.write.mode(\"overwrite\").parquet(emp_location)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"employees_DFv2 = spark.read.parquet(emp_location)\r\n",
							"department_DFv2 = spark.read.parquet(dept_location)\r\n",
							"\r\n",
							"employees_DFv2.show()\r\n",
							"department_DFv2.show()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from hyperspace import *\r\n",
							"\r\n",
							"hyperspace = Hyperspace(spark)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"employee_indexConfig = IndexConfig(\"empIndex1\", [\"deptId\"], [\"empName\"])\r\n",
							"department_indexConfig = IndexConfig(\"deptIndex1\", [\"deptId\"], [\"deptName\"])\r\n",
							"department_indexConfig1 = IndexConfig(\"deptIndex2\", [\"location\"], [\"deptName\"])"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.createIndex(employees_DFv2, employee_indexConfig)\r\n",
							"hyperspace.createIndex(department_DFv2, department_indexConfig)\r\n",
							"hyperspace.createIndex(department_DFv2, department_indexConfig1)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.deleteIndex(\"deptIndex2\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.restoreIndex(\"deptIndex2\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.deleteIndex(\"deptIndex2\")"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.vacuumIndex(\"deptIndex2\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.enable(spark)\r\n",
							"\r\n",
							"hyperspace.disable(spark)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.enable(spark)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"e_DF = spark.read.parquet(emp_location)\r\n",
							"d_DF = spark.read.parquet(dept_location)\r\n",
							"\r\n",
							"e_DF.show(5)\r\n",
							"d_DF.show(5)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"eqFilter = d_DF.filter(\"\"\"deptId = 12\"\"\").select(\"\"\"deptName\"\"\")\r\n",
							"\r\n",
							"eqFilter.show()\r\n",
							"\r\n",
							"hyperspace.explain(eqFilter, True, displayHTML)"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"d_DF.createOrReplaceTempView(\"DEPT\")"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT deptName\r\n",
							"FROM DEPT\r\n",
							"WHERE deptId = 12"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"query = spark.sql(\"\"\"SELECT deptName FROM DEPT WHERE deptId = 12\"\"\")\r\n",
							"\r\n",
							"hyperspace.explain(query, True, displayHTML)"
						],
						"outputs": [],
						"execution_count": 29
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ML Synapse Integration')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool001v1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a3bd0296-ecce-4c9d-b12c-6a6e8687934d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/924cf5c7-5f72-42ba-98ee-7ea905016982/resourceGroups/trainingresourcegroupv10/providers/Microsoft.Synapse/workspaces/trainingsynapseresources/bigDataPools/SparkPool001v1",
						"name": "SparkPool001v1",
						"type": "Spark",
						"endpoint": "https://trainingsynapseresources.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool001v1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\r\n",
							"from sklearn.datasets import fetch_california_housing\r\n",
							"\r\n",
							"california_housing = fetch_california_housing()\r\n",
							"pd_df_california_housing = pd.DataFrame(california_housing.data,\r\n",
							"    columns = california_housing.feature_names)\r\n",
							"pd_df_california_housing[\"target\"] = pd.Series(california_housing.target)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pd_df_california_housing_inferencing = pd_df_california_housing.sample(frac=0.01)\r\n",
							"pd_df_california_housing_training = pd_df_california_housing.drop(pd_df_california_housing_inferencing.index)\r\n",
							"pd_df_california_housing_inferencing[\"target\"] = 0.0"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark_df_california_housing_inferencing = spark.createDataFrame(pd_df_california_housing_inferencing)\r\n",
							"spark_df_california_housing_training = spark.createDataFrame(pd_df_california_housing_training)\r\n",
							"\r\n",
							"spark_df_california_housing_inferencing.createOrReplaceTempView(\"CaliforniaInference\")\r\n",
							"spark_df_california_housing_training.write.mode(\"overwrite\").saveAsTable(\"default.CaliforniaTrain\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\r\n",
							"\r\n",
							"val scala_df_train = spark.sqlContext.sql(\"SELECT * FROM CaliforniaTrain\")\r\n",
							"\r\n",
							"scala_df_train\r\n",
							"    .write\r\n",
							"    .synapsesql(\"SQLPool01v1.dbo.CaliforniaTrain\", Constants.INTERNAL)\r\n",
							"\r\n",
							"val scala_df_inference = spark.sqlContext.sql(\"SELECT * FROM CaliforniaInference\")\r\n",
							"\r\n",
							"scala_df_inference\r\n",
							"    .write\r\n",
							"    .synapsesql(\"SQLPool01v1.dbo.CaliforniaInference\", Constants.INTERNAL)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool002v24",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fe00ae83-7b63-4b65-836d-211f7a6e8516"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/924cf5c7-5f72-42ba-98ee-7ea905016982/resourceGroups/trainingresourcegroupv10/providers/Microsoft.Synapse/workspaces/trainingsynapseresources/bigDataPools/SparkPool002v24",
						"name": "SparkPool002v24",
						"type": "Spark",
						"endpoint": "https://trainingsynapseresources.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool002v24",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import azureml.core\n",
							"\n",
							"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
							"from azureml.train.automl import AutoMLConfig\n",
							"from notebookutils import mssparkutils\n",
							"from azureml.data.dataset_factory import TabularDatasetFactory"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"linkedService_name = \"AzureMLService1\"\n",
							"experiment_name = \"trainingsynapseresources-californiatrain-20220623111249\"\n",
							"\n",
							"ws = mssparkutils.azureML.getWorkspace(linkedService_name)\n",
							"experiment = Experiment(ws, experiment_name)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.sql(\"SELECT * FROM default.californiatrain\")\n",
							"\n",
							"datastore = Datastore.get_default(ws)\n",
							"dataset = TabularDatasetFactory.register_spark_dataframe(df, datastore, name = experiment_name + \"-dataset\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"automl_config = AutoMLConfig(spark_context = sc,\n",
							"                             task = \"regression\",\n",
							"                             training_data = dataset,\n",
							"                             label_column_name = \"target\",\n",
							"                             primary_metric = \"r2_score\",\n",
							"                             experiment_timeout_hours = 0.5,\n",
							"                             max_concurrent_iterations = 2,\n",
							"                             enable_onnx_compatible_models = True)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"run = experiment.submit(automl_config)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"run.wait_for_completion()\n",
							"\n",
							"import onnxruntime\n",
							"import mlflow\n",
							"import mlflow.onnx\n",
							"\n",
							"from mlflow.models.signature import ModelSignature\n",
							"from mlflow.types import DataType\n",
							"from mlflow.types.schema import ColSpec, Schema\n",
							"\n",
							"# Get best model from automl run\n",
							"best_run, onnx_model = run.get_output(return_onnx_model=True)\n",
							"\n",
							"# Define utility functions to infer the schema of ONNX model\n",
							"def _infer_schema(data):\n",
							"    res = []\n",
							"    for _, col in enumerate(data):\n",
							"        t = col.type.replace(\"tensor(\", \"\").replace(\")\", \"\")\n",
							"        if t in [\"bool\"]:\n",
							"            dt = DataType.boolean\n",
							"        elif t in [\"int8\", \"uint8\", \"int16\", \"uint16\", \"int32\"]:\n",
							"            dt = DateType.integer\n",
							"        elif t in [\"uint32\", \"int64\"]:\n",
							"            dt = DataType.long\n",
							"        elif t in [\"float16\", \"bfloat16\", \"float\"]:\n",
							"            dt = DataType.float\n",
							"        elif t in [\"double\"]:\n",
							"            dt = DataType.double\n",
							"        elif t in [\"string\"]:\n",
							"            dt = DataType.string\n",
							"        else:\n",
							"            raise Exception(\"Unsupported type: \" + t)\n",
							"        res.append(ColSpec(type=dt, name=col.name))\n",
							"    return Schema(res)\n",
							"\n",
							"def _infer_signature(onnx_model):\n",
							"    onnx_model_bytes = onnx_model.SerializeToString()\n",
							"    onnx_runtime = onnxruntime.InferenceSession(onnx_model_bytes)\n",
							"    inputs = _infer_schema(onnx_runtime.get_inputs())\n",
							"    outputs = _infer_schema(onnx_runtime.get_outputs())\n",
							"    return ModelSignature(inputs, outputs)\n",
							"\n",
							"# Infer signature of ONNX model\n",
							"signature = _infer_signature(onnx_model)\n",
							"\n",
							"artifact_path = experiment_name + \"_artifact\"\n",
							"mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
							"mlflow.set_experiment(experiment_name)\n",
							"\n",
							"with mlflow.start_run() as run:\n",
							"    # Save the model to the outputs directory for capture\n",
							"    mlflow.onnx.log_model(onnx_model, artifact_path, signature=signature)\n",
							"\n",
							"    # Register the model to AML model registry\n",
							"    mlflow.register_model(\"runs:/\" + run.info.run_id + \"/\" + artifact_path, \"trainingsynapseresources-californiatrain-20220623111249-Best\")"
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Single Threaded Notebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool001v1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d4317309-b23a-48dd-8ef7-a5a8f6ee169e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/924cf5c7-5f72-42ba-98ee-7ea905016982/resourceGroups/trainingresourcegroupv10/providers/Microsoft.Synapse/workspaces/trainingsynapseresources/bigDataPools/SparkPool001v1",
						"name": "SparkPool001v1",
						"type": "Spark",
						"endpoint": "https://trainingsynapseresources.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool001v1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import numpy as np\r\n",
							"import pandas as pd\r\n",
							"\r\n",
							"from sklearn.datasets import load_boston\r\n",
							"\r\n",
							"boston = load_boston()\r\n",
							"\r\n",
							"boston_pd = pd \\\r\n",
							"    .DataFrame( \\\r\n",
							"        data = np.c_[boston[\"data\"], boston[\"target\"]], \\\r\n",
							"        columns = np.append(boston[\"feature_names\"], \"target\")) \\\r\n",
							"    .sample(frac = 1)\r\n",
							"\r\n",
							"print(boston_pd.shape)\r\n",
							"\r\n",
							"boston_pd.head(5)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from sklearn.linear_model import LinearRegression\r\n",
							"from scipy.stats.stats import pearsonr\r\n",
							"\r\n",
							"y = boston_pd[\"target\"]\r\n",
							"X = boston_pd.drop([\"target\"], axis = 1)\r\n",
							"\r\n",
							"X_train = X[:400]\r\n",
							"X_test = X[400:]\r\n",
							"\r\n",
							"y_train = y[:400]\r\n",
							"y_test = y[400:]\r\n",
							"\r\n",
							"lr = LinearRegression()\r\n",
							"model = lr.fit(X_train, y_train)\r\n",
							"\r\n",
							"y_pred = model.predict(X_test)\r\n",
							"\r\n",
							"r = pearsonr(y_pred, y_test)\r\n",
							"\r\n",
							"mae = sum(abs(y_pred - y_test)) / len(y_test)\r\n",
							"\r\n",
							"print(\"R-Squared : \" + str(r[0] ** 2))\r\n",
							"print(\"MAE : \" + str(mae))"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Learning Native Spark Library"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.ml.feature import VectorAssembler\r\n",
							"\r\n",
							"boston_sp = spark.createDataFrame(boston_pd)\r\n",
							"display(boston_sp.take(5))"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"boston_train = spark.createDataFrame(boston_pd[:400])\r\n",
							"boston_test = spark.createDataFrame(boston_pd[400:])\r\n",
							"\r\n",
							"assembler = VectorAssembler(\r\n",
							"    inputCols = boston_train.schema.names[:(boston_pd.shape[1]-1)],\r\n",
							"    outputCol = \"features\"\r\n",
							")\r\n",
							"\r\n",
							"boston_train = assembler.transform(boston_train).select(\"features\", \"target\")\r\n",
							"boston_test = assembler.transform(boston_test).select(\"features\", \"target\")\r\n",
							"\r\n",
							"display(boston_train.take(5))"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.ml.regression import LinearRegression\r\n",
							"\r\n",
							"lr = LinearRegression(\r\n",
							"    maxIter = 10,\r\n",
							"    regParam = 0.1,\r\n",
							"    elasticNetParam = 0.5,\r\n",
							"    labelCol = \"target\")\r\n",
							"\r\n",
							"model = lr.fit(boston_train)\r\n",
							"\r\n",
							"boston_pred = model.transform(boston_test)\r\n",
							"\r\n",
							"r = boston_pred.stat.corr(\"prediction\", \"target\")\r\n",
							"\r\n",
							"print(\"R-Squared : \" + str(r**2))"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Learning Thread Pools"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from sklearn.ensemble import RandomForestRegressor as RFR\r\n",
							"from multiprocessing.pool import ThreadPool\r\n",
							"\r\n",
							"pool = ThreadPool(4)\r\n",
							"\r\n",
							"parameters = [10,20,50]\r\n",
							"\r\n",
							"def sklearn_random_forest(trees, X_train, X_test, y_train, y_test):\r\n",
							"    rf = RFR(n_estimators=trees)\r\n",
							"    model = rf.fit(X_train, y_train)\r\n",
							"\r\n",
							"    y_pred = model.predict(X_test)\r\n",
							"    r = pearsonr(y_pred, y_test)\r\n",
							"\r\n",
							"    return [trees, r[0]**2]\r\n",
							"\r\n",
							"pool.map(lambda trees: sklearn_random_forest(trees, X_train, X_test, y_train, y_test), parameters)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.ml.regression import RandomForestRegressor\r\n",
							"\r\n",
							"def mllib_random_forest(trees, boston_train, boston_test):\r\n",
							"    rf = RandomForestRegressor(numTrees=trees, labelCol=\"target\")\r\n",
							"    model = rf.fit(boston_train)\r\n",
							"    boston_pred = model.transform(boston_test)\r\n",
							"    r = boston_pred.stat.corr(\"prediction\", \"target\")\r\n",
							"\r\n",
							"    return [trees, r**2]\r\n",
							"\r\n",
							"pool.map(lambda trees: mllib_random_forest(trees, boston_train, boston_test), parameters)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Learning Pandas UDFs"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import PandasUDFType, pandas_udf\r\n",
							"from pyspark.sql.types import *\r\n",
							"\r\n",
							"boston_sp.createOrReplaceTempView(\"boston\")\r\n",
							"\r\n",
							"full_df = spark.sql(\"\"\"\r\n",
							"\r\n",
							"SELECT *\r\n",
							"FROM (\r\n",
							"    SELECT *, CASE WHEN rand() < 0.8 THEN 1 ELSE 0 END AS training\r\n",
							"    FROM boston\r\n",
							") b\r\n",
							"CROSS JOIN (\r\n",
							"    SELECT 11 AS trees\r\n",
							"    UNION ALL SELECT 20 AS trees\r\n",
							"    UNION ALL SELECT 50 AS trees\r\n",
							")\r\n",
							"\r\n",
							"\"\"\")\r\n",
							"\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"trees\", LongType(), True),\r\n",
							"    StructField(\"r_squared\", DoubleType(), True)])\r\n",
							"\r\n",
							"@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\r\n",
							"def train_RF(boston_pd):\r\n",
							"    trees = boston_pd['trees'].unique()[0]\r\n",
							"\r\n",
							"    boston_train = boston_pd[boston_pd[\"training\"] == 1]\r\n",
							"    boston_test = boston_pd[boston_pd[\"training\"] == 0]\r\n",
							"\r\n",
							"    y_train = boston_train[\"target\"]\r\n",
							"    X_train = boston_train.drop([\"target\"], axis = 1)\r\n",
							"\r\n",
							"    y_test = boston_test[\"target\"]\r\n",
							"    X_test = boston_test.drop([\"target\"], axis = 1)\r\n",
							"\r\n",
							"    rf = RFR(n_estimators = trees)\r\n",
							"    model = rf.fit(X_train, y_train)\r\n",
							"    y_pred = model.predict(X_test)\r\n",
							"    r = pearsonr(y_pred, y_test)\r\n",
							"\r\n",
							"    return pd.DataFrame({\r\n",
							"        'trees': trees,\r\n",
							"        'r_squared': (r[0] ** 2)\r\n",
							"    }, index = [0])\r\n",
							"\r\n",
							"\r\n",
							"results = full_df.groupby(\"trees\").apply(train_RF)   \r\n",
							"\r\n",
							"print(results.take(3))"
						],
						"outputs": [],
						"execution_count": 18
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Stream Ingestion to Cosmos')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool001v1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0666ed82-d2f4-46df-aa3a-2e6c24bf68c7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/924cf5c7-5f72-42ba-98ee-7ea905016982/resourceGroups/trainingresourcegroupv10/providers/Microsoft.Synapse/workspaces/trainingsynapseresources/bigDataPools/SparkPool001v1",
						"name": "SparkPool001v1",
						"type": "Spark",
						"endpoint": "https://trainingsynapseresources.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool001v1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"dfStream = ( \\\r\n",
							"    spark \\\r\n",
							"        .readStream \\\r\n",
							"        .format(\"rate\") \\\r\n",
							"        .option(\"rowsPerSecond\", 10)\r\n",
							"        .load()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pyspark.sql.functions as F\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"import uuid\r\n",
							"\r\n",
							"numberOfDevices = 10\r\n",
							"generate_uuid = F.udf(lambda : str(uuid.uuid4()), StringType())\r\n",
							"\r\n",
							"dfIoTSignals = (dfStream \r\n",
							"    .withColumn(\"id\", generate_uuid())\r\n",
							"    .withColumn(\"deviceId\", F.concat(F.lit(\"dev-\"), F.expr(\"mod(value, %d)\" % numberOfDevices)))\r\n",
							"    .withColumn(\"dateTime\", dfStream[\"timestamp\"].cast(StringType()))\r\n",
							"    .withColumn(\"unit\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Revolutions Per Minute' ELSE 'Megawatts' END\"))\r\n",
							"    .withColumn(\"unitSymbol\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'RPM' ELSE 'MW' END\"))\r\n",
							"    .withColumn(\"measureType\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Rotation Speed' ELSE 'Output' END\"))\r\n",
							"    .withColumn(\"measureValue\", F.expr(\"CASE WHEN rand() > 0.95 THEN value * 10 WHEN rand() < 0.05 THEN value div 10 ELSE value END\"))\r\n",
							"    .drop(\"timestamp\", \"value\"))"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"streamQuery = \\\r\n",
							"    dfIoTSignals \\\r\n",
							"        .writeStream \\\r\n",
							"        .format(\"cosmos.oltp\") \\\r\n",
							"        .outputMode(\"append\") \\\r\n",
							"        .option(\"spark.synapse.linkedService\", \"CosmosDb1\") \\\r\n",
							"        .option(\"spark.cosmos.container\", \"IoTSignals\") \\\r\n",
							"        .option(\"checkpointLocation\", \"/tmp/checkpoint-1\") \\\r\n",
							"        .start()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"streamQuery.status"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"streamQuery.awaitTermination()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"streamQuery.stop()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"path = \"/IoTDeviceInfo.csv\"\r\n",
							"\r\n",
							"dfDeviceInfo = (spark.read.csv(path, header = True, inferSchema=True)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dfDeviceInfo \\\r\n",
							"    .write \\\r\n",
							"    .format(\"cosmos.oltp\") \\\r\n",
							"    .option(\"spark.synapse.linkedService\", \"CosmosDb1\") \\\r\n",
							"    .option(\"spark.cosmos.container\", \"IoTDeviceInfo\") \\\r\n",
							"    .option(\"spark.cosmos.write.upsertEnabled\", \"true\") \\\r\n",
							"    .start()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Training Spark Configuration')]",
			"type": "Microsoft.Synapse/workspaces/sparkConfigurations",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"configs": {
					"spark.buffer.size": "65536"
				},
				"created": "2022-06-23T10:14:52.9600000+05:30",
				"createdBy": "ramkumar@exonia64.onmicrosoft.com",
				"annotations": [],
				"configMergeRule": {
					"artifact.currentOperation.spark.buffer.size": "replace"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SparkPool001v1')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 0,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SparkPool002v24')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "2.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLPool01v1')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}