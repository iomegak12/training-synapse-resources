{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "trainingsynapseresources"
		},
		"trainingsynapseresources-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'trainingsynapseresources-WorkspaceDefaultSqlServer'"
		},
		"trainingstoragev101_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://trainingstoragev101.dfs.core.windows.net/"
		},
		"trainingsynapseresources-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://synapsedlsv10.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/trainingstoragev101')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('trainingstoragev101_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/trainingsynapseresources-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('trainingsynapseresources-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/trainingsynapseresources-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('trainingsynapseresources-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TrainingDataFlowLibrary')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "UDFLibrary",
				"typeProperties": {
					"sources": [],
					"sinks": [],
					"transformations": [],
					"scriptLines": [
						"ExConcat(string, string) as string = concat(i1,i2)"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Databricks - Query Performance with Parquet and Delta')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool001v1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0f07fe79-db9b-4be5-a9fa-2651c3f9b53d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/924cf5c7-5f72-42ba-98ee-7ea905016982/resourceGroups/trainingresourcegroupv10/providers/Microsoft.Synapse/workspaces/trainingsynapseresources/bigDataPools/SparkPool001v1",
						"name": "SparkPool001v1",
						"type": "Spark",
						"endpoint": "https://trainingsynapseresources.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool001v1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"mssparkutils.fs.mount(\r\n",
							"    \"abfss://data@trainingstoragev101.dfs.core.windows.net/\",\r\n",
							"    \"/test\",\r\n",
							"    {\r\n",
							"        \"linkedService\": \"trainingstoragev101\"\r\n",
							"    }\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jobId = mssparkutils.env.getJobId()\r\n",
							"path = \"synfs:/\" + jobId + \"/test/data.csv\"\r\n",
							"\r\n",
							"print(path)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"flights = spark \\\r\n",
							"    .read \\\r\n",
							"    .format(\"csv\") \\\r\n",
							"    .option(\"header\", \"true\") \\\r\n",
							"    .option(\"inferSchema\", \"true\") \\\r\n",
							"    .load(path)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"flights.printSchema()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"flights \\\r\n",
							"    .write \\\r\n",
							"    .format(\"parquet\") \\\r\n",
							"    .mode(\"overwrite\") \\\r\n",
							"    .partitionBy(\"Origin\") \\\r\n",
							"    .save(\"/tmp/flights_parquet\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import count\r\n",
							"\r\n",
							"flights_parquet = \\\r\n",
							"    spark \\\r\n",
							"        .read \\\r\n",
							"        .format(\"parquet\") \\\r\n",
							"        .load(\"/tmp/flights_parquet\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(\r\n",
							"    flights_parquet\r\n",
							"        .filter(\"DayOfWeek = 1\")\r\n",
							"        .groupBy(\"Month\", \"Origin\")\r\n",
							"        .agg(count(\"*\").alias(\"TotalFlights\"))\r\n",
							"        .orderBy(\"TotalFlights\", ascending = False)\r\n",
							"        .limit(20)\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"flights \\\r\n",
							"    .write \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .mode(\"overwrite\") \\\r\n",
							"    .partitionBy(\"Origin\") \\\r\n",
							"    .save(\"/tmp/flights_delta\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"DROP TABLE IF EXISTS flights;"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"CREATE TABLE flights\r\n",
							"USING DELTA\r\n",
							"LOCATION '/tmp/flights_delta';"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"OPTIMIZE flights ZORDER BY (DayOfWeek);"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"flights_deltav2 = \\\r\n",
							"    spark \\\r\n",
							"        .read \\\r\n",
							"        .format(\"delta\") \\\r\n",
							"        .load('/tmp/flights_delta')"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(\r\n",
							"    flights_deltav2\r\n",
							"        .filter(\"DayOfWeek = 1\")\r\n",
							"        .groupBy(\"Month\", \"Origin\")\r\n",
							"        .agg(count(\"*\").alias(\"TotalFlights\"))\r\n",
							"        .orderBy(\"TotalFlights\", ascending = False)\r\n",
							"        .limit(20)\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta Lake Exploration')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool001v1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0c64b8f1-b44c-4bcb-8198-daa35f44e04b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/924cf5c7-5f72-42ba-98ee-7ea905016982/resourceGroups/trainingresourcegroupv10/providers/Microsoft.Synapse/workspaces/trainingsynapseresources/bigDataPools/SparkPool001v1",
						"name": "SparkPool001v1",
						"type": "Spark",
						"endpoint": "https://trainingsynapseresources.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool001v1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import random\r\n",
							"\r\n",
							"session_id = random.randint(1,1000000)\r\n",
							"delta_table_path = \"/delta-training/delta-table-{0}\".format(session_id)\r\n",
							"\r\n",
							"delta_table_path"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data = spark.range(0,10)\r\n",
							"data.show()\r\n",
							"\r\n",
							"data \\\r\n",
							"    .write \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark \\\r\n",
							"    .read \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .load(delta_table_path)\r\n",
							"\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data = spark.range(10, 20)\r\n",
							"data \\\r\n",
							"    .write \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .mode(\"overwrite\") \\\r\n",
							"    .save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data \\\r\n",
							"    .write \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .saveAsTable(\"ManagedDeltaTablev2\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"CREATE TABLE ExternalDeltaTablev2\r\n",
							"USING DELTA\r\n",
							"LOCATION '/delta-training/delta-table-645199'"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SHOW TABLES"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"DESCRIBE EXTENDED manageddeltatablev2"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"delta_table.update(\r\n",
							"    condition = expr (\"id % 2 == 1\"),\r\n",
							"    set = { \"id\": expr(\"id + 100\")}\r\n",
							")\r\n",
							"\r\n",
							"delta_table.delete(\"id >= 9\")\r\n",
							"\r\n",
							"delta_table.toDF().show()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data = spark.range(10, 20)\r\n",
							"data \\\r\n",
							"    .write \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .mode(\"overwrite\") \\\r\n",
							"    .save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"new_data = spark.range(15, 25)\r\n",
							"\r\n",
							"delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"\r\n",
							"delta_table \\\r\n",
							"    .alias(\"oldData\") \\\r\n",
							"    .merge(new_data.alias(\"newData\"), \"oldData.id = newData.id\") \\\r\n",
							"    .whenMatchedUpdate(set = {'id': lit(-1)}) \\\r\n",
							"    .whenNotMatchedInsert(values = { 'id': col(\"newData.id\") }) \\\r\n",
							"    .execute()\r\n",
							"\r\n",
							"delta_table.toDF().show()"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"delta_table \\\r\n",
							"    .history() \\\r\n",
							"    .show()"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark \\\r\n",
							"    .read \\\r\n",
							"    .format (\"delta\") \\\r\n",
							"    .option(\"versionAsOf\", 0) \\\r\n",
							"    .load(delta_table_path)\r\n",
							"\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"streaming_df = \\\r\n",
							"    spark \\\r\n",
							"        .readStream \\\r\n",
							"        .format(\"rate\") \\\r\n",
							"        .load()\r\n",
							"    \r\n",
							"stream = streaming_df \\\r\n",
							"    .selectExpr(\"value as id\") \\\r\n",
							"    .writeStream \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .option(\"checkpointLocation\", \"/tmp/checkpoint-{0}\".format(session_id)) \\\r\n",
							"    .start(delta_table_path)\r\n",
							"\r\n",
							"stream.status"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"delta_table \\\r\n",
							"    .toDF() \\\r\n",
							"    .sort(col(\"id\").desc()) \\\r\n",
							"    .show(100)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"delta_table \\\r\n",
							"    .history() \\\r\n",
							"    .show()"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stream.stop()"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"partition_count = 2\r\n",
							"\r\n",
							"spark \\\r\n",
							"    .read \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .load(delta_table_path) \\\r\n",
							"    .repartition(partition_count) \\\r\n",
							"    .write \\\r\n",
							"    .option(\"dataChange\", \"false\") \\\r\n",
							"    .mode(\"overwrite\") \\\r\n",
							"    .save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"DESCRIBE HISTORY ManagedDeltaTablev2\").show()"
						],
						"outputs": [],
						"execution_count": 26
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta Other Optimization Techniques')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool001v1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "97f4eca0-6ce1-4426-a634-21255ae3b6f0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/924cf5c7-5f72-42ba-98ee-7ea905016982/resourceGroups/trainingresourcegroupv10/providers/Microsoft.Synapse/workspaces/trainingsynapseresources/bigDataPools/SparkPool001v1",
						"name": "SparkPool001v1",
						"type": "Spark",
						"endpoint": "https://trainingsynapseresources.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool001v1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"mssparkutils.fs.mount(\r\n",
							"    \"abfss://data@trainingstoragev101.dfs.core.windows.net/\",\r\n",
							"    \"/test\",\r\n",
							"    {\r\n",
							"        \"linkedService\": \"trainingstoragev101\"\r\n",
							"    }\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jobId = mssparkutils.env.getJobId()\r\n",
							"path = \"synfs:/\" + jobId + \"/test/Common/Orders/parquetFiles\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.ls(path)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import count, year\r\n",
							"\r\n",
							"ordersDF = spark \\\r\n",
							"    .read \\\r\n",
							"    .format(\"parquet\") \\\r\n",
							"    .load(path) \\\r\n",
							"    .withColumn(\"OrderYear\", year(\"O_ORDERDATE\"))\r\n",
							"\r\n",
							"display(ordersDF.limit(5))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(\r\n",
							"    ordersDF\r\n",
							"        .filter(\"O_ORDERSTATUS = 'O'\")\r\n",
							"        .groupBy(\"OrderYear\", \"O_ORDERPRIORITY\")\r\n",
							"        .agg(count(\"*\").alias(\"TotalOrders\"))\r\n",
							"        .orderBy(\"TotalOrders\", ascending = False)\r\n",
							"        .limit(20)\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ordersDF \\\r\n",
							"    .write \\\r\n",
							"    .format(\"delta\") \\\r\n",
							"    .mode(\"overwrite\") \\\r\n",
							"    .partitionBy(\"OrderYear\") \\\r\n",
							"    .save(\"/tmp/orders-delta\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"CREATE TABLE Orders\r\n",
							"USING DELTA\r\n",
							"LOCATION \"/tmp/orders-delta\""
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"OPTIMIZE Orders ZORDER BY (O_ORDERPRIORITY)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT COUNT(*) AS TotalOrders FROM Orders\r\n",
							"WHERE O_ORDERSTATUS = 'O'\r\n",
							"GROUP BY OrderYear, O_ORDERPRIORITY\r\n",
							"ORDER BY TotalOrders DESC\r\n",
							"LIMIT 20"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"VACUUM Orders "
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"DESCRIBE HISTORY Orders"
						],
						"outputs": [],
						"execution_count": 17
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Training Spark Configuration')]",
			"type": "Microsoft.Synapse/workspaces/sparkConfigurations",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"configs": {
					"spark.buffer.size": "65536"
				},
				"created": "2022-06-23T10:14:52.9600000+05:30",
				"createdBy": "ramkumar@exonia64.onmicrosoft.com",
				"annotations": [],
				"configMergeRule": {
					"artifact.currentOperation.spark.buffer.size": "replace"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SparkPool001v1')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 0,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLPool01v1')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Hyperspace Integration with Spark Pool')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool001v1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3a796592-7515-4c03-8bed-2589e99e2428"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/924cf5c7-5f72-42ba-98ee-7ea905016982/resourceGroups/trainingresourcegroupv10/providers/Microsoft.Synapse/workspaces/trainingsynapseresources/bigDataPools/SparkPool001v1",
						"name": "SparkPool001v1",
						"type": "Spark",
						"endpoint": "https://trainingsynapseresources.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool001v1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import random\r\n",
							"session_id = random.randint(1,1000000)\r\n",
							"data_path = \"/hyperspace/data-{0}\".format(session_id)\r\n",
							"index_location = \"/hyperspace/indexes-{0}\".format(session_id)\r\n",
							"\r\n",
							"spark.conf.set(\"spark.hyperspace.system.path\", index_location)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\r\n",
							"spark.conf.set(\"spark.hyperspace.explain.displayMode\", \"html\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.types import *\r\n",
							"\r\n",
							"departments = [(10, \"Accounting\", \"Bangalore\"), (11, \"Marketing\", \"Singapore\"), (12, \"IT\", \"New York\"), (13, \"HR\", \"Bangalore\"),\r\n",
							"    (14, \"Personnel\", \"Chennai\")]\r\n",
							"\r\n",
							"employees = [(1, \"Ramkumar\", 10), \r\n",
							"    (2, \"Rajkumar\", 10),\r\n",
							"    (3, \"Rajesh\", 11),\r\n",
							"    (4, \"Mahesh\", 10),\r\n",
							"    (5, \"Mukhesh\", 10),\r\n",
							"    (6, \"Vikram\", 12),\r\n",
							"    (7, \"Lilly\", 12),\r\n",
							"    (8, \"Sophana\", 13),\r\n",
							"    (9, \"Brown\", 10),\r\n",
							"    (10, \"Mathais\", 14)]\r\n",
							"\r\n",
							"depart_schema = StructType([\r\n",
							"    StructField(\"deptId\", IntegerType(), True),\r\n",
							"    StructField(\"deptName\", StringType(), True),\r\n",
							"    StructField(\"location\", StringType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"emp_schema = StructType([\r\n",
							"    StructField(\"empId\", IntegerType(), True),\r\n",
							"    StructField(\"empName\", StringType(), True),\r\n",
							"    StructField(\"deptId\", IntegerType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"departments_df = spark.createDataFrame(departments, depart_schema)\r\n",
							"employees_df = spark.createDataFrame(employees, emp_schema)\r\n",
							"\r\n",
							"dept_location = data_path + \"/departments.parquet\"\r\n",
							"emp_location = data_path + \"/employees.parquet\"\r\n",
							"\r\n",
							"departments_df.write.mode(\"overwrite\").parquet(dept_location)\r\n",
							"employees_df.write.mode(\"overwrite\").parquet(emp_location)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"employees_DFv2 = spark.read.parquet(emp_location)\r\n",
							"department_DFv2 = spark.read.parquet(dept_location)\r\n",
							"\r\n",
							"employees_DFv2.show()\r\n",
							"department_DFv2.show()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from hyperspace import *\r\n",
							"\r\n",
							"hyperspace = Hyperspace(spark)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"employee_indexConfig = IndexConfig(\"empIndex1\", [\"deptId\"], [\"empName\"])\r\n",
							"department_indexConfig = IndexConfig(\"deptIndex1\", [\"deptId\"], [\"deptName\"])\r\n",
							"department_indexConfig1 = IndexConfig(\"deptIndex2\", [\"location\"], [\"deptName\"])"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.createIndex(employees_DFv2, employee_indexConfig)\r\n",
							"hyperspace.createIndex(department_DFv2, department_indexConfig)\r\n",
							"hyperspace.createIndex(department_DFv2, department_indexConfig1)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.deleteIndex(\"deptIndex2\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.restoreIndex(\"deptIndex2\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.deleteIndex(\"deptIndex2\")"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.vacuumIndex(\"deptIndex2\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.enable(spark)\r\n",
							"\r\n",
							"hyperspace.disable(spark)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.enable(spark)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"e_DF = spark.read.parquet(emp_location)\r\n",
							"d_DF = spark.read.parquet(dept_location)\r\n",
							"\r\n",
							"e_DF.show(5)\r\n",
							"d_DF.show(5)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"eqFilter = d_DF.filter(\"\"\"deptId = 12\"\"\").select(\"\"\"deptName\"\"\")\r\n",
							"\r\n",
							"eqFilter.show()\r\n",
							"\r\n",
							"hyperspace.explain(eqFilter, True, displayHTML)"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"d_DF.createOrReplaceTempView(\"DEPT\")"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT deptName\r\n",
							"FROM DEPT\r\n",
							"WHERE deptId = 12"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"query = spark.sql(\"\"\"SELECT deptName FROM DEPT WHERE deptId = 12\"\"\")\r\n",
							"\r\n",
							"hyperspace.explain(query, True, displayHTML)"
						],
						"outputs": [],
						"execution_count": 29
					}
				]
			},
			"dependsOn": []
		}
	]
}